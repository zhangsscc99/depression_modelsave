{"cells":[{"cell_type":"code","execution_count":2,"metadata":{"trusted":true},"outputs":[],"source":["import numpy as np\n","import pandas as pd\n","from pathlib import Path\n","import os.path\n","\n","from sklearn.model_selection import train_test_split\n","\n","import tensorflow as tf\n","\n","from sklearn.metrics import r2_score"]},{"cell_type":"code","execution_count":35,"metadata":{"trusted":true},"outputs":[],"source":["image_dir = Path('/Users/mac/Downloads/githubdepression/githubdepression/implementation/AV2014_Training_data_NYU/depression_score')"]},{"cell_type":"code","execution_count":36,"metadata":{"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["<ipython-input-36-f6cc017ce478>:2: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n","Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n","  depression = pd.Series(filepaths.apply(lambda x: os.path.split(os.path.split(x)[0])[1]), name='Depression').astype(np.int)\n"]}],"source":["filepaths = pd.Series(list(image_dir.glob(r'**/*.jpg')), name='Filepath').astype(str)\n","depression = pd.Series(filepaths.apply(lambda x: os.path.split(os.path.split(x)[0])[1]), name='Depression').astype(np.int)\n","\n","images = pd.concat([filepaths, depression], axis=1).sample(frac=1.0, random_state=1).reset_index(drop=True)"]},{"cell_type":"code","execution_count":37,"metadata":{"trusted":true},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>Filepath</th>\n","      <th>Depression</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>/Users/mac/Downloads/githubdepression/githubde...</td>\n","      <td>7</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>/Users/mac/Downloads/githubdepression/githubde...</td>\n","      <td>9</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>/Users/mac/Downloads/githubdepression/githubde...</td>\n","      <td>18</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>/Users/mac/Downloads/githubdepression/githubde...</td>\n","      <td>6</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>/Users/mac/Downloads/githubdepression/githubde...</td>\n","      <td>5</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>3646</th>\n","      <td>/Users/mac/Downloads/githubdepression/githubde...</td>\n","      <td>2</td>\n","    </tr>\n","    <tr>\n","      <th>3647</th>\n","      <td>/Users/mac/Downloads/githubdepression/githubde...</td>\n","      <td>11</td>\n","    </tr>\n","    <tr>\n","      <th>3648</th>\n","      <td>/Users/mac/Downloads/githubdepression/githubde...</td>\n","      <td>29</td>\n","    </tr>\n","    <tr>\n","      <th>3649</th>\n","      <td>/Users/mac/Downloads/githubdepression/githubde...</td>\n","      <td>18</td>\n","    </tr>\n","    <tr>\n","      <th>3650</th>\n","      <td>/Users/mac/Downloads/githubdepression/githubde...</td>\n","      <td>29</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>3651 rows × 2 columns</p>\n","</div>"],"text/plain":["                                               Filepath  Depression\n","0     /Users/mac/Downloads/githubdepression/githubde...           7\n","1     /Users/mac/Downloads/githubdepression/githubde...           9\n","2     /Users/mac/Downloads/githubdepression/githubde...          18\n","3     /Users/mac/Downloads/githubdepression/githubde...           6\n","4     /Users/mac/Downloads/githubdepression/githubde...           5\n","...                                                 ...         ...\n","3646  /Users/mac/Downloads/githubdepression/githubde...           2\n","3647  /Users/mac/Downloads/githubdepression/githubde...          11\n","3648  /Users/mac/Downloads/githubdepression/githubde...          29\n","3649  /Users/mac/Downloads/githubdepression/githubde...          18\n","3650  /Users/mac/Downloads/githubdepression/githubde...          29\n","\n","[3651 rows x 2 columns]"]},"execution_count":37,"metadata":{},"output_type":"execute_result"}],"source":["images"]},{"cell_type":"code","execution_count":39,"metadata":{"trusted":true},"outputs":[],"source":["# Let's only use 5000 images to speed up training time\n","#image_df = images.sample(5000, random_state=1).reset_index(drop=True)\n","image_df = images.sample(2000, random_state=1).reset_index(drop=True)\n","\n","train_df, test_df = train_test_split(image_df, train_size=0.7, shuffle=True, random_state=1)"]},{"cell_type":"code","execution_count":40,"metadata":{"trusted":true},"outputs":[],"source":["train_generator = tf.keras.preprocessing.image.ImageDataGenerator(\n","    rescale=1./255,\n","    validation_split=0.2\n",")\n","\n","test_generator = tf.keras.preprocessing.image.ImageDataGenerator(\n","    rescale=1./255\n",")"]},{"cell_type":"code","execution_count":41,"metadata":{"trusted":true},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>Filepath</th>\n","      <th>Depression</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>1194</th>\n","      <td>/Users/mac/Downloads/githubdepression/githubde...</td>\n","      <td>16</td>\n","    </tr>\n","    <tr>\n","      <th>45</th>\n","      <td>/Users/mac/Downloads/githubdepression/githubde...</td>\n","      <td>17</td>\n","    </tr>\n","    <tr>\n","      <th>1477</th>\n","      <td>/Users/mac/Downloads/githubdepression/githubde...</td>\n","      <td>3</td>\n","    </tr>\n","    <tr>\n","      <th>1293</th>\n","      <td>/Users/mac/Downloads/githubdepression/githubde...</td>\n","      <td>17</td>\n","    </tr>\n","    <tr>\n","      <th>1736</th>\n","      <td>/Users/mac/Downloads/githubdepression/githubde...</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>1791</th>\n","      <td>/Users/mac/Downloads/githubdepression/githubde...</td>\n","      <td>2</td>\n","    </tr>\n","    <tr>\n","      <th>1096</th>\n","      <td>/Users/mac/Downloads/githubdepression/githubde...</td>\n","      <td>29</td>\n","    </tr>\n","    <tr>\n","      <th>1932</th>\n","      <td>/Users/mac/Downloads/githubdepression/githubde...</td>\n","      <td>17</td>\n","    </tr>\n","    <tr>\n","      <th>235</th>\n","      <td>/Users/mac/Downloads/githubdepression/githubde...</td>\n","      <td>24</td>\n","    </tr>\n","    <tr>\n","      <th>1061</th>\n","      <td>/Users/mac/Downloads/githubdepression/githubde...</td>\n","      <td>21</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>1400 rows × 2 columns</p>\n","</div>"],"text/plain":["                                               Filepath  Depression\n","1194  /Users/mac/Downloads/githubdepression/githubde...          16\n","45    /Users/mac/Downloads/githubdepression/githubde...          17\n","1477  /Users/mac/Downloads/githubdepression/githubde...           3\n","1293  /Users/mac/Downloads/githubdepression/githubde...          17\n","1736  /Users/mac/Downloads/githubdepression/githubde...           0\n","...                                                 ...         ...\n","1791  /Users/mac/Downloads/githubdepression/githubde...           2\n","1096  /Users/mac/Downloads/githubdepression/githubde...          29\n","1932  /Users/mac/Downloads/githubdepression/githubde...          17\n","235   /Users/mac/Downloads/githubdepression/githubde...          24\n","1061  /Users/mac/Downloads/githubdepression/githubde...          21\n","\n","[1400 rows x 2 columns]"]},"execution_count":41,"metadata":{},"output_type":"execute_result"}],"source":["train_df"]},{"cell_type":"code","execution_count":42,"metadata":{"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Found 1120 validated image filenames.\n","Found 280 validated image filenames.\n","Found 600 validated image filenames.\n"]}],"source":["train_images = train_generator.flow_from_dataframe(\n","    dataframe=train_df,\n","    x_col='Filepath',\n","    y_col='Depression',\n","    target_size=(120, 120),\n","    color_mode='rgb',\n","    class_mode='raw',\n","    batch_size=32,\n","    shuffle=True,\n","    seed=42,\n","    subset='training'\n",")\n","\n","val_images = train_generator.flow_from_dataframe(\n","    dataframe=train_df,\n","    x_col='Filepath',\n","    y_col='Depression',\n","    target_size=(120, 120),\n","    color_mode='rgb',\n","    class_mode='raw',\n","    batch_size=32,\n","    shuffle=True,\n","    seed=42,\n","    subset='validation'\n",")\n","\n","test_images = test_generator.flow_from_dataframe(\n","    dataframe=test_df,\n","    x_col='Filepath',\n","    y_col='Depression',\n","    target_size=(120, 120),\n","    color_mode='rgb',\n","    class_mode='raw',\n","    batch_size=32,\n","    shuffle=False\n",")"]},{"cell_type":"markdown","metadata":{},"source":["# Training"]},{"cell_type":"code","execution_count":43,"metadata":{"trusted":true},"outputs":[{"data":{"text/plain":["\"\\ninputs = tf.keras.Input(shape=(120, 120, 3))\\nx = tf.keras.layers.Conv2D(filters=16, kernel_size=(3, 3), activation='relu')(inputs)\\nx = tf.keras.layers.MaxPool2D()(x)\\nx = tf.keras.layers.Conv2D(filters=32, kernel_size=(3, 3), activation='relu')(x)\\nx = tf.keras.layers.MaxPool2D()(x)\\nx = tf.keras.layers.GlobalAveragePooling2D()(x)\\nx = tf.keras.layers.Dense(64, activation='relu')(x)\\nx = tf.keras.layers.Dense(64, activation='relu')(x)\\noutputs = tf.keras.layers.Dense(1, activation='linear')(x)\\n\\nmodel = tf.keras.Model(inputs=inputs, outputs=outputs)\\n\\nmodel.compile(\\n    optimizer='adam',\\n    loss='mse'\\n)\\n\\nhistory = model.fit(\\n    train_images,\\n    validation_data=val_images,\\n    epochs=1,\\n    callbacks=[\\n        tf.keras.callbacks.EarlyStopping(\\n            monitor='val_loss',\\n            patience=5,\\n            restore_best_weights=True\\n        )\\n    ]\\n)\\n\""]},"execution_count":43,"metadata":{},"output_type":"execute_result"}],"source":["\"\"\"\n","inputs = tf.keras.Input(shape=(120, 120, 3))\n","x = tf.keras.layers.Conv2D(filters=16, kernel_size=(3, 3), activation='relu')(inputs)\n","x = tf.keras.layers.MaxPool2D()(x)\n","x = tf.keras.layers.Conv2D(filters=32, kernel_size=(3, 3), activation='relu')(x)\n","x = tf.keras.layers.MaxPool2D()(x)\n","x = tf.keras.layers.GlobalAveragePooling2D()(x)\n","x = tf.keras.layers.Dense(64, activation='relu')(x)\n","x = tf.keras.layers.Dense(64, activation='relu')(x)\n","outputs = tf.keras.layers.Dense(1, activation='linear')(x)\n","\n","model = tf.keras.Model(inputs=inputs, outputs=outputs)\n","\n","model.compile(\n","    optimizer='adam',\n","    loss='mse'\n",")\n","\n","history = model.fit(\n","    train_images,\n","    validation_data=val_images,\n","    epochs=1,\n","    callbacks=[\n","        tf.keras.callbacks.EarlyStopping(\n","            monitor='val_loss',\n","            patience=5,\n","            restore_best_weights=True\n","        )\n","    ]\n",")\n","\"\"\""]},{"cell_type":"code","execution_count":44,"metadata":{},"outputs":[],"source":["from keras.models import Model\n","from keras.layers import Input, Conv2D, MaxPooling2D, AveragePooling2D, Flatten, GlobalAveragePooling2D, Dense, Dropout\n","from tensorflow.keras.layers import concatenate"]},{"cell_type":"code","execution_count":45,"metadata":{},"outputs":[],"source":["def Inception_block(input_layer, f1, f2_conv1, f2_conv3, f3_conv1, f3_conv5, f4): \n","  # Input: \n","  # - f1: number of filters of the 1x1 convolutional layer in the first path\n","  # - f2_conv1, f2_conv3 are number of filters corresponding to the 1x1 and 3x3 convolutional layers in the second path\n","  # - f3_conv1, f3_conv5 are the number of filters corresponding to the 1x1 and 5x5  convolutional layer in the third path\n","  # - f4: number of filters of the 1x1 convolutional layer in the fourth path\n","\n","  # 1st path:\n","  path1 = Conv2D(filters=f1, kernel_size = (1,1), padding = 'same', activation = 'relu')(input_layer)\n","\n","  # 2nd path\n","  path2 = Conv2D(filters = f2_conv1, kernel_size = (1,1), padding = 'same', activation = 'relu')(input_layer)\n","  path2 = Conv2D(filters = f2_conv3, kernel_size = (3,3), padding = 'same', activation = 'relu')(path2)\n","\n","  # 3rd path\n","  path3 = Conv2D(filters = f3_conv1, kernel_size = (1,1), padding = 'same', activation = 'relu')(input_layer)\n","  path3 = Conv2D(filters = f3_conv5, kernel_size = (5,5), padding = 'same', activation = 'relu')(path3)\n","\n","  # 4th path\n","  path4 = MaxPooling2D((3,3), strides= (1,1), padding = 'same')(input_layer)\n","  path4 = Conv2D(filters = f4, kernel_size = (1,1), padding = 'same', activation = 'relu')(path4)\n","\n","  output_layer = concatenate([path1, path2, path3, path4], axis = -1)\n","\n","  return output_layer"]},{"cell_type":"code","execution_count":46,"metadata":{},"outputs":[],"source":["def GoogLeNet():\n","      # input layer \n","  input_layer = Input(shape = (120, 120, 3))\n","\n","  # convolutional layer: filters = 64, kernel_size = (7,7), strides = 2\n","  X = Conv2D(filters = 64, kernel_size = (7,7), strides = 2, padding = 'valid', activation = 'relu')(input_layer)\n","\n","  # max-pooling layer: pool_size = (3,3), strides = 2\n","  X = MaxPooling2D(pool_size = (3,3), strides = 2)(X)\n","\n","  # convolutional layer: filters = 64, strides = 1\n","  X = Conv2D(filters = 64, kernel_size = (1,1), strides = 1, padding = 'same', activation = 'relu')(X)\n","\n","  \n","\n","  # convolutional layer: filters = 192, kernel_size = (3,3)\n","  X = Conv2D(filters = 192, kernel_size = (3,3), padding = 'same', activation = 'relu')(X)\n","\n","  # max-pooling layer: pool_size = (3,3), strides = 2\n","  X = MaxPooling2D(pool_size= (3,3), strides = 2)(X)\n","\n","  # 1st Inception block\n","  X = Inception_block(X, f1 = 64, f2_conv1 = 96, f2_conv3 = 128, f3_conv1 = 16, f3_conv5 = 32, f4 = 32)\n","\n","  # 2nd Inception block\n","  X = Inception_block(X, f1 = 128, f2_conv1 = 128, f2_conv3 = 192, f3_conv1 = 32, f3_conv5 = 96, f4 = 64)\n","\n","  # max-pooling layer: pool_size = (3,3), strides = 2\n","  X = MaxPooling2D(pool_size= (3,3), strides = 2)(X)\n","\n","  # 3rd Inception block\n","  X = Inception_block(X, f1 = 192, f2_conv1 = 96, f2_conv3 = 208, f3_conv1 = 16, f3_conv5 = 48, f4 = 64)\n","\n","  # Extra network 1:\n","  X1 = AveragePooling2D(pool_size = (5,5), strides = 3)(X)\n","  X1 = Conv2D(filters = 128, kernel_size = (1,1), padding = 'same', activation = 'relu')(X1)\n","  X1 = Flatten()(X1)\n","  X1 = Dense(1024, activation = 'relu')(X1)\n","  X1 = Dropout(0.7)(X1)\n","  X1 = Dense(5, activation = 'softmax')(X1)\n","\n","  \n","  # 4th Inception block\n","  X = Inception_block(X, f1 = 160, f2_conv1 = 112, f2_conv3 = 224, f3_conv1 = 24, f3_conv5 = 64, f4 = 64)\n","\n","  # 5th Inception block\n","  X = Inception_block(X, f1 = 128, f2_conv1 = 128, f2_conv3 = 256, f3_conv1 = 24, f3_conv5 = 64, f4 = 64)\n","\n","  # 6th Inception block\n","  X = Inception_block(X, f1 = 112, f2_conv1 = 144, f2_conv3 = 288, f3_conv1 = 32, f3_conv5 = 64, f4 = 64)\n","\n","  # Extra network 2:\n","  X2 = AveragePooling2D(pool_size = (5,5), strides = 3)(X)\n","  X2 = Conv2D(filters = 128, kernel_size = (1,1), padding = 'same', activation = 'relu')(X2)\n","  X2 = Flatten()(X2)\n","  X2 = Dense(1024, activation = 'relu')(X2)\n","  X2 = Dropout(0.7)(X2)\n","  X2 = Dense(1000, activation = 'softmax')(X2)\n","  \n","  \n","  # 7th Inception block\n","  X = Inception_block(X, f1 = 256, f2_conv1 = 160, f2_conv3 = 320, f3_conv1 = 32, \n","                      f3_conv5 = 128, f4 = 128)\n","\n","  # max-pooling layer: pool_size = (3,3), strides = 2\n","  X = MaxPooling2D(pool_size = (3,3), strides = 2)(X)\n","\n","  # 8th Inception block\n","  X = Inception_block(X, f1 = 256, f2_conv1 = 160, f2_conv3 = 320, f3_conv1 = 32, f3_conv5 = 128, f4 = 128)\n","\n","  # 9th Inception block\n","  X = Inception_block(X, f1 = 384, f2_conv1 = 192, f2_conv3 = 384, f3_conv1 = 48, f3_conv5 = 128, f4 = 128)\n","\n","  # Global Average pooling layer \n","  X = GlobalAveragePooling2D(name = 'GAPL')(X)\n","\n","  # Dropoutlayer \n","  X = Dropout(0.4)(X)\n","\n","  # output layer \n","  X = Dense(1, activation = 'linear')(X)\n","  model = Model(input_layer, X, name = 'GoogLeNet')\n","\n","  return model"]},{"cell_type":"code","execution_count":47,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["35/35 [==============================] - 27s 680ms/step - loss: 156.3513 - val_loss: 60.3607\n"]}],"source":["model = GoogLeNet()\n","\n","model.compile(\n","    optimizer='adam',\n","    loss='mse'\n",")\n","\n","history = model.fit(\n","    train_images,\n","    validation_data=val_images,\n","    epochs=1,\n","    callbacks=[\n","        tf.keras.callbacks.EarlyStopping(\n","            monitor='val_loss',\n","            patience=5,\n","            restore_best_weights=True\n","        )\n","    ]\n",")"]},{"cell_type":"markdown","metadata":{},"source":["# Results"]},{"cell_type":"code","execution_count":48,"metadata":{"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["19/19 [==============================] - 6s 260ms/step\n","     Test RMSE: 7.65487\n","Test R^2 Score: -0.04493\n"]}],"source":["predicted_depression = np.squeeze(model.predict(test_images))\n","true_depression = test_images.labels\n","\n","rmse = np.sqrt(model.evaluate(test_images, verbose=0))\n","print(\"     Test RMSE: {:.5f}\".format(rmse))\n","\n","\n","r2 = r2_score(true_depression, predicted_depression)\n","print(\"Test R^2 Score: {:.5f}\".format(r2))"]},{"cell_type":"code","execution_count":49,"metadata":{"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Null/Baseline Model Test RMSE: 7.48848\n"]}],"source":["null_rmse = np.sqrt(np.sum((true_depression - np.mean(true_depression))**2) / len(true_depression))\n","print(\"Null/Baseline Model Test RMSE: {:.5f}\".format(null_rmse))"]}],"metadata":{"kernelspec":{"display_name":"Python 3.8.5 ('base')","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.5"},"vscode":{"interpreter":{"hash":"40d3a090f54c6569ab1632332b64b2c03c39dcf918b08424e98f38b5ae0af88f"}}},"nbformat":4,"nbformat_minor":4}
